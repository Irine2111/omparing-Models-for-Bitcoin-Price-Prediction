{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Literature Review (72 points)\n",
    "\n",
    "(II)Chen, Zheshi, Chunhong Li, and Wenjun Sun. \"Bitcoin price prediction using machine learning: An approach to sample dimension engineering.\" Journal of Computational and Applied Mathematics 365 (2020): 112395.\n",
    "\n",
    "\n",
    "For each paper chosen, answer the following questions:\n",
    "1. What venue was the paper published in? Who are the authors? What are their backgrounds? How many times has the paper been cited? (~3-5 sentences)\n",
    "\n",
    "    The paper is published in the Journal of Computational and Applied Mathematics. The authors are Zheshi Chen , Chunhong Li, Wenjun Sun . The paper has been cited by 65 times.\n",
    "\n",
    "2. What problem are the authors trying to solve? (~2 sentences)\n",
    "\n",
    "    The problem that the authors tried to solve is whether different modeling algorithms have different feasibility when apply to different data structures and dimensional features. Also, which machine learning algorithm will have the best performance for the daily price and 5-minute-price respectively.\n",
    "\n",
    "3. Why is the problem important (2-5 sentences)?\n",
    "\n",
    "    Bitcoin has been increasingly regarded as an investment asset in recent years during the boom of cryptocurrencies. However, at the same time, the investment decisions of these new investment assets need to base on reliable predictions because of their highly volatile nature. Though many studies concentrated on machine learning algorithms that could do more accurate predictions, few focused on the feasibility of different models when applied to different structured data. So, the study about the necessity of sample dimension in machine learning algorithms is extremely important.\n",
    "\n",
    "4. What mathematical notation do the authors use and what does this notation mean? This is often most useful represented as a table.\n",
    "\n",
    "t:\tIndex of time\n",
    "v:\tIndex of feature\n",
    "c:\tCurrent time\n",
    "h:\tPrediction length\n",
    "g:\tGranularity (5 min and 60*24min)\n",
    "Xg=[Xvt]:\tXg denotes the matrix of training samples under granularity g, xvt ∈ R denotes the value of the vth feature at time t \n",
    "Yg={yt}g: \tYg denotes the vector of labels, y ∈ {−1, 1} denotes the value of the label at time t. Here yt = −1 indicates that the price drops, while yt = 1 indicates that the price increases \n",
    "Lg:  Prediction error\n",
    "\n",
    "5. Make a list of five terms that you do not understand from your paper. For each, do a bit of research and write a brief description of the term. (~2 sentences each.)\n",
    "\n",
    "    (1)sh rate : The estimated number of the hashes per second (trillions of hashes per second) the Bitcoin network is performing.\n",
    "    (2)Mining difficulty: A relative measure of how difficult it is to find a new block. The difficulty is adjusted periodically as a function of how much hashing power has been deployed by the network of miners。 \n",
    "    (3)Mempool transaction count: The number of transactions waiting to be confirmed\n",
    "    (4)Gold spot price: XAU gold spot price in US dollars. \n",
    "    (5)Market capitalization: The total US dollar market value of Bitcoin.\n",
    "\n",
    "6. What datasets did the authors use (if any)? Why did they use these datasets? (~2 sentences)\n",
    "    The authors used two datasets. One includes the aggregated Bitcoin daily price data, property, and network data, trading and market data, media and investor attention, and gold spot price, for the period from February 2, 2017, to February 1, 2019. from CoinMarketCap.com. The second dataset consists of 5-minute interval Bitcoin real-time trading price data, trading volume, open, close, high, and low points from July 17, 2017, to January 17, 2018, at high-frequency and large scale pulled from Binance.\n",
    "\t1. What machine learning models did the authors use (if any)? Why did they justify using these? (~2 sentences)\n",
    "        The authors implement two statistical methods for Bitcoin daily price with higher dimensional features, t: logistic regression (LR), which is a traditional multiple variate regression method that can be implemented in binary classifications, and linear discriminant analysis (LDA) which can reduce the dimensionality of data and for classification purposes. For the 5-minute interval price with fewer features, the authors used random forest (RF), which is an ensemble of decision trees for various tasks to obtain a better classification result, XGBoost (XGB), which is a framework and library that parallelizes the growth of gradient boosted trees in a forest, quadratic discriminant analysis (QDA), which is a kind of distance discrimination method for supervised classification problems, support vector machine (SVM), which is a kind of machine learning methodology that is applied in binary classification problems, and long short-term memory (LSTM), which uses a memory cell and gate to solve the problem of the gradient disappearance of an RNN.\n",
    "\t2. How did the authors know their approach was successful? (~10 sentences)\n",
    "        The authors construct a confusion matrix and divide the results into four categories. The confusion matrix contains statistics about real classification data and the predictions generated by these seven classifier algorithms. the authors imported true/false positive and true/false negative. They calculated accuracy and F statistics to estimate the performance of the approaches.\n",
    "7. What evaluation metrics did they use? What do these metrics mean?\n",
    "    They used a confusion matrix. The rows and columns in the confusion matrix show the instances of real and predicted classes.\n",
    "\t1. What baselines did they compare against?\n",
    "        \n",
    "        The result of logistic regression is served as the benchmark against which other algorithms are compared.\n",
    "        \n",
    "\t2. Can you identify a problem with the authors' measure of success?\n",
    "        \n",
    "        I currently could not think about a problem with the authors’ measure of success.\n",
    "\n",
    "8.  Research papers are almost always an improvement, reaction or twist on other research (“prior work”) that others have done before. Of all of the works cited in your paper, what seems to be the most important cited prior work?Explain why that citation seems most important. (4-6 sentences)\n",
    "\n",
    "    I think the most important cited prior work is D. Gamberger, N. Lavrač, Conditions for Occam's razor applicability and noise elimination, in European Conference on Machine Learning, Springer, 1997. This prior work suggests a proposition that the simplest model can be used when leveraging machine learning if assuming all other criteria are equal. This proposition is controversial but is supported in general. So, based on this work, the authors assumed simple statistical methods are capable to predict Bitcoin price and included them in the analyses.\n",
    "\n",
    "9.  Research papers almost always make use of tools, methods and algorithms that have been developed by others. Of all of the works cited in your paper, what citation of a tool, method or algorithm seems most important? Explain why. (4-6 sentences)\n",
    "\n",
    "    I think L. Kristoufek, What are the main drivers of the Bitcoin price? evidence from wavelet coherence analysis, PLoS One 10 (4) (2015) e0123923 is the most important. This work investigated the drivers of Bitcoin price including fundamental sources to speculative and technical ones and the distinction between short-term and long-term connections. The authors included the features that drive the price of Bitcoin price in their analysis as this prior work mentioned. They leveraged Google Trends search volume index and Baidu media search volume. \n",
    "\n",
    "\n",
    "10. Pick one equation from the paper and explain what it means using a mix of prose and mathematical notation (~5 sentences).\n",
    "\n",
    "    The paper used F statistics to measure the performance of each model. The equation of F statistics is F 1-score = 2 × Precision × Recall/(Precision + Recall) . Precision is a metric that quantifies the number of correct positive predictions made and is calculated as the number of true positives divided by the total number of true positives and false positives (Precision = tp/(tp + fP)). Recall is a metric that quantifies the number of correct positive predictions made out of all positive predictions that could have been made (Recall = tp/(tp + fn)). where tp denotes a true positive, tn denotes a true negative, fp denotes a false positive, and fn denotes a false negative. \n",
    "\n",
    "11. How is your project similar and different from this paper? (4-5 sentences)\n",
    "\n",
    "    Similar to this paper, our project wants to investigate how different models perform on data with different periods. Also, we want to know which models outperform others when applied to a certain kind of dataset. However, we want to test this in the different featured datasets. We would use other predicting factors in our project and see whether the result would be different.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
